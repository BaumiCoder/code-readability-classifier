= Predicting Source Code Readability

This Python program utilizes machine learning to predict the readability of source code snippets.
It is designed to work with Python 3.11.5 and uses Poetry for package management.

== Table of Contents

* <<Installation>>
* <<Usage>>
** <<Predict>>
** <<Train>>
** <<Dataset>>

[[Installation]]
== Installation

To set up the project and its dependencies, follow these steps:

Clone this repository to your local machine:

[source,bash]
----
git clone https://github.com/LuKrO2011/readability-classifier
cd readability-classifier
----

Install Poetry if you haven't already:

[source,bash]
----
pip install poetry
----

Create a virtual environment and install the project's dependencies using Poetry:

[source,bash]
----
poetry install
----

Activate the virtual environment:

[source,bash]
----
poetry shell
----

Install tensorflow manually:

----
pip install tensorflow
----

For Developers: Activate the pre-commit hooks:
----
pre-commit install
----

Now you're ready to use the source code readability prediction tool.

[[Usage]]
== Usage

[[Predict]]
=== Predict

To predict the readability of a source code snippet, use the following command:

[source,bash]
----
python src/readability_classifier/main.py PREDICT --model MODEL --input INPUT [--token-length TOKEN_LENGTH]
----

* `--model` or `-m`: Path to the pre-trained machine learning model.
* `--input` or `-i`: Path to the source code snippet you want to evaluate.
* `--token-length` or `-l` (optional): The token length of the snippet (cutting/padding applied).

Example:

[source,bash]
----
python src/readability_classifier/main.py PREDICT --model tests/res/models/model.pt --input <snippet_path>
----

[[Train]]
=== Train

To train a new machine learning model for source code readability prediction, use the following command:

[source,bash]
----
python src/readability_classifier/main.py TRAIN --input INPUT [--save SAVE] [--intermediate INTERMEDIATE] [--evaluate] [--token-length TOKEN_LENGTH] [--batch-size BATCH_SIZE] [--epochs EPOCHS] [--learning-rate LEARNING_RATE]
----

* `--input` or `-i`: Path to the dataset containing a folder 'Snippets' and 'scores.csv'.
* `--save` or `-s` (optional): Path to the folder where the trained model should be stored.
If not specified, the model is not stored.
* `--intermediate` (optional): Path to the folder where intermediate results should be stored.
If not specified, the intermediate results are not stored.
* `--evaluate` (optional): Whether to evaluate the model after training.
* `--token-length` or `-l` (optional): The token length of the snippets (cutting/padding applied).
* `--batch-size` or `-b` (optional): The batch size for training.
* `--epochs` or `-e` (optional): The number of epochs for training.
* `--learning-rate` or `-r` (optional): The learning rate for training.

Example:

[source,bash]
----
python src/readability_classifier/main.py TRAIN --input tests/res/raw_datasets/bw --save <output-path>
----

[[Dataset]]
== Dataset

The datasets used for training and evaluation are from the following sources:

* BW: Raymond PL Buse and Westley R Weimer.
‘Learning a metric for code readability’
* Dorn: Jonathan Dorn.
‘A general software readability model’.
* Scalabrio: Simone Scalabrino et al.
‘Improving code readability models with textual features’.
* Krod: TODO

In order to use the datasets with the model, they need to be converted into link:https://huggingface.co/docs/datasets/index[Hugging Face's Dataset format].
This can be done using the dataset_converter.py script.
Datasets can be combined arbitrarily using the dataset_combiner.py script.
A command line interface for those will come soon.

[[Podman]]
== Podman

To build the podman container, run the following command:

[source,bash]
----
podman build -t readability-classifier .
----

- t : name of the container
- . : path to the Dockerfile

To run the podman container, run the following command:

[source,bash]
----
podman run -it --rm -v $(pwd):/app readability-classifier
----

- it : interactive mode
- rm : remove container after exit
- v $(pwd):/app : mount current directory to /app in container
- readability-classifier : name of the container

[[Model]]
== Model

|===
|Layer (type) |Output Shape              |Param # |Connected to

|struc_input (InputLayer) |[(None, 50, 305)]         |0       |[]
|struc_reshape (Reshape) |(None, 50, 305, 1)         |0       |['struc_input[0][0]']
|vis_input (InputLayer) |[(None, 128, 128, 3)]      |0       |[]
|struc_conv1 (Conv2D) |(None, 48, 303, 32)         |320     |['struc_reshape[0][0]']
|vis_conv1 (Conv2D) |(None, 128, 128, 32)        |896     |['vis_input[0][0]']
|struc_pool1 (MaxPooling2D) |(None, 24, 151, 32)    |0       |['struc_conv1[0][0]']
|seman_input_token (InputLayer) |[(None, 100)]    |0       |[]
|seman_input_segment (InputLayer) |[(None, 100)] |0       |[]
|vis_pool1 (MaxPooling2D) |(None, 64, 64, 32)        |0       |['vis_conv1[0][0]']
|struc_conv2 (Conv2D) |(None, 22, 149, 32)          |9248    |['struc_pool1[0][0]']
|seman_bert (BertEmbedding) |(None, 100, 768)       |2342553 |['seman_input_token[0][0]', 'seman_input_segment[0][0]']
|vis_conv2 (Conv2D) |(None, 64, 64, 32)           |9248    |['vis_pool1[0][0]']
|struc_pool2 (MaxPooling2D) |(None, 11, 74, 32)      |0       |['struc_conv2[0][0]']
|seman_conv1 (Conv1D) |(None, 96, 32)               |122912 |['seman_bert[0][0]']
|vis_pool2 (MaxPooling2D) |(None, 32, 32, 32)        |0       |['vis_conv2[0][0]']
|struc_conv3 (Conv2D) |(None, 9, 72, 64)            |18496  |['struc_pool2[0][0]']
|seman_pool1 (MaxPooling1D) |(None, 32, 32)          |0       |['seman_conv1[0][0]']
|vis_conv3 (Conv2D) |(None, 32, 32, 64)           |18496  |['vis_pool2[0][0]']
|struc_pool3 (MaxPooling2D) |(None, 3, 24, 64)        |0       |['struc_conv3[0][0]']
|seman_conv2 (Conv1D) |(None, 28, 32)               |5152   |['seman_pool1[0][0]']
|vis_pool3 (MaxPooling2D) |(None, 16, 16, 64)        |0       |['vis_conv3[0][0]']
|struc_flatten (Flatten) |(None, 4608)               |0       |['struc_pool3[0][0]']
|seman_gru (Bidirectional) |(None, 64)               |16640  |['seman_conv2[0][0]']
|vis_flatten (Flatten) |(None, 16384)                |0       |['vis_pool3[0][0]']
|concatenate (Concatenate) |(None, 21056)             |0       |['struc_flatten[0][0]', 'seman_gru[0][0]', 'vis_flatten[0][0]']
|class_dense1 (Dense) |(None, 64)                   |1347648 |['concatenate[0][0]']
|class_dropout (Dropout) |(None, 64)                 |0       |['class_dense1[0][0]']
|class_dense2 (Dense) |(None, 16)                   |1040   |['class_dropout[0][0]']
|class_dense3 (Dense) |(None, 1)                    |17     |['class_dense2[0][0]']

|===
Total params: 24975649 (95.27 MB)

--freeze struc_input struc_reshape struc_conv1 struc_pool1 seman_input_token seman_input_segment seman_bert seman_conv1 seman_pool1 vis_input vis_conv1 vis_pool1

--freeze struc_input struc_reshape seman_input_token seman_input_segment seman_bert vis_input
